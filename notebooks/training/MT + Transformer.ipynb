{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch.optim as optim\n",
    "import math, time\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus\", split=\"train\").select(range(30000)).remove_columns([\"id\", \"model1_accepted\", \"model2_accepted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'And everyone will not care that it is not you.',\n",
       " 'japanese': '鼻・口のところはあらかじめ少し切っておくといいですね。'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'english'\n",
    "TRG_LANGUAGE = 'japanese'\n",
    "\n",
    "data_list = [(item[SRC_LANGUAGE], item[TRG_LANGUAGE]) for item in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = type('CustomDataset', (Dataset,), {\n",
    "    '__len__': lambda self: len(data_list),\n",
    "    '__getitem__': lambda self, idx: data_list[idx]\n",
    "})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - (train_size + val_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 29001 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('And everyone will not care that it is not you.',\n",
       " '鼻・口のところはあらかじめ少し切っておくといいですね。')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(dataset))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='ja_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  And everyone will not care that it is not you.\n",
      "Tokenization:  ['And', 'everyone', 'will', 'not', 'care', 'that', 'it', 'is', 'not', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", sample[0])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[433, 19, 11, 0, 11]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visual'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9728"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, ja in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([12, 81])\n",
      "Japanese shape:  torch.Size([12, 91])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Japanese shape: \", ja.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        #energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(9728, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(9301, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=9301, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2490368\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2381056\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2381056\n",
      "  9301\n",
      "______\n",
      "11266645\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos. \n",
    "        try:\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "        except:\n",
    "            continue\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "       \n",
    "            try:\n",
    "                output, _ = model(src, trg[:,:-1])\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 16m 10s\n",
      "Train Loss: 1.545 | Train PPL:   4.688\n",
      "Val. Loss: 0.906 |  Val. PPL:   2.474\n",
      "Epoch: 02 | Time: 32m 35s\n",
      "Train Loss: 0.899 | Train PPL:   2.456\n",
      "Val. Loss: 0.827 |  Val. PPL:   2.286\n",
      "Epoch: 03 | Time: 49m 49s\n",
      "Train Loss: 0.810 | Train PPL:   2.249\n",
      "Val. Loss: 0.796 |  Val. PPL:   2.218\n",
      "Epoch: 04 | Time: 65m 35s\n",
      "Train Loss: 0.739 | Train PPL:   2.093\n",
      "Val. Loss: 0.773 |  Val. PPL:   2.165\n",
      "Epoch: 05 | Time: 82m 33s\n",
      "Train Loss: 0.661 | Train PPL:   1.937\n",
      "Val. Loss: 0.770 |  Val. PPL:   2.159\n",
      "Epoch: 06 | Time: 101m 39s\n",
      "Train Loss: 0.649 | Train PPL:   1.914\n",
      "Val. Loss: 0.761 |  Val. PPL:   2.140\n",
      "Epoch: 07 | Time: 118m 36s\n",
      "Train Loss: 0.602 | Train PPL:   1.826\n",
      "Val. Loss: 0.765 |  Val. PPL:   2.149\n",
      "Epoch: 08 | Time: 134m 12s\n",
      "Train Loss: 0.564 | Train PPL:   1.757\n",
      "Val. Loss: 0.764 |  Val. PPL:   2.148\n",
      "Epoch: 09 | Time: 149m 26s\n",
      "Train Loss: 0.519 | Train PPL:   1.681\n",
      "Val. Loss: 0.775 |  Val. PPL:   2.170\n",
      "Epoch: 10 | Time: 166m 28s\n",
      "Train Loss: 0.485 | Train PPL:   1.623\n",
      "Val. Loss: 0.792 |  Val. PPL:   2.207\n",
      "Epoch: 11 | Time: 183m 20s\n",
      "Train Loss: 0.464 | Train PPL:   1.591\n",
      "Val. Loss: 0.790 |  Val. PPL:   2.204\n",
      "Epoch: 12 | Time: 198m 26s\n",
      "Train Loss: 0.434 | Train PPL:   1.544\n",
      "Val. Loss: 0.810 |  Val. PPL:   2.247\n",
      "Epoch: 13 | Time: 215m 14s\n",
      "Train Loss: 0.415 | Train PPL:   1.514\n",
      "Val. Loss: 0.825 |  Val. PPL:   2.281\n",
      "Epoch: 14 | Time: 231m 25s\n",
      "Train Loss: 0.385 | Train PPL:   1.470\n",
      "Val. Loss: 0.833 |  Val. PPL:   2.300\n",
      "Epoch: 15 | Time: 241m 44s\n",
      "Train Loss: 0.383 | Train PPL:   1.467\n",
      "Val. Loss: 0.842 |  Val. PPL:   2.321\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 15\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'../../app/models/transformer/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(vocab_transform[SRC_LANGUAGE].get_default_index())  # Should be UNK_IDX (0)\n",
    "print(vocab_transform[TRG_LANGUAGE].get_default_index())  # Should be UNK_IDX (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src index: tensor(9550)\n",
      "Max trg index: tensor(8252)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    src, _, trg = batch\n",
    "    print(\"Max src index:\", torch.max(src))\n",
    "    print(\"Max trg index:\", torch.max(trg))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab size: 9728\n",
      "Target vocab size: 9301\n"
     ]
    }
   ],
   "source": [
    "print(f\"Source vocab size: {len(vocab_transform[SRC_LANGUAGE])}\")\n",
    "print(f\"Target vocab size: {len(vocab_transform[TRG_LANGUAGE])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dim: 9728, Output dim: 9301\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input dim: {INPUT_DIM}, Output dim: {OUTPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.810 | Test PPL:   2.249 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And everyone will not care that it is not you.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'鼻・口のところはあらかじめ少し切っておくといいですね。'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  306,  656,   54,   90, 1520,   28,   37,   19,   90,   17,    5,\n",
       "           3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2, 3969,   64,  823,    4, 1097,   10, 2470, 1072, 2666,    9, 1091,\n",
       "          13,  300,   27, 1038,    8,    3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13]), torch.Size([1, 18]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text,    ) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 9301])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 9301])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 9301])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 33,   0,  14, 235,  14,   5,   0,  14,   9,  42,  13, 512,   8,   8,\n",
       "          8,   3,   8])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "から\n",
      "<unk>\n",
      "で\n",
      "場合\n",
      "で\n",
      "、\n",
      "<unk>\n",
      "で\n",
      "て\n",
      "いる\n",
      "と\n",
      "思い\n",
      "。\n",
      "。\n",
      "。\n",
      "<eos>\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 18, 13])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 13])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'And',\n",
       " 'everyone',\n",
       " 'will',\n",
       " 'not',\n",
       " 'care',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'you',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'から',\n",
       " '<unk>',\n",
       " 'で',\n",
       " '場合',\n",
       " 'で',\n",
       " '、',\n",
       " '<unk>',\n",
       " 'で',\n",
       " 'て',\n",
       " 'いる',\n",
       " 'と',\n",
       " '思い',\n",
       " '。',\n",
       " '。',\n",
       " '。',\n",
       " '<eos>',\n",
       " '。']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swara\\AppData\\Local\\Temp\\ipykernel_8560\\140266900.py:15: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\swara\\AppData\\Local\\Temp\\ipykernel_8560\\140266900.py:16: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12363 (\\N{HIRAGANA LETTER KA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12425 (\\N{HIRAGANA LETTER RA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12391 (\\N{HIRAGANA LETTER DE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 22580 (\\N{CJK UNIFIED IDEOGRAPH-5834}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 21512 (\\N{CJK UNIFIED IDEOGRAPH-5408}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12289 (\\N{IDEOGRAPHIC COMMA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12390 (\\N{HIRAGANA LETTER TE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12356 (\\N{HIRAGANA LETTER I}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12427 (\\N{HIRAGANA LETTER RU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12392 (\\N{HIRAGANA LETTER TO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 24605 (\\N{CJK UNIFIED IDEOGRAPH-601D}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12290 (\\N{IDEOGRAPHIC FULL STOP}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../../app/models/transformer/model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(f\"../../app/models/transformer/vocab_transform.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab_transform, f)\n",
    "\n",
    "with open(f\"../../app/models/transformer/token_transform.pkl\", \"wb\") as f:\n",
    "    pickle.dump(token_transform, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../helper/transformer/text_transform.pkl\", \"wb\") as f:\n",
    "    dill.dump(text_transform, f)\n",
    "\n",
    "with open(f\"../helper/transformer/train_loader.pkl\", \"wb\") as f:\n",
    "    dill.dump(train_loader, f)\n",
    "\n",
    "with open(f\"../helper/transformer/val_loader.pkl\", \"wb\") as f:\n",
    "    dill.dump(valid_loader, f)\n",
    "\n",
    "with open(f\"../helper/transformer/test_loader.pkl\", \"wb\") as f:\n",
    "    dill.dump(test_loader, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
