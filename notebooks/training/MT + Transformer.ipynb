{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation + Transformer\n",
    "\n",
    "<img src = \"../figures/transformer1.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import torch.optim as optim\n",
    "import math, time\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "#make our work comparable if restarted the kernel\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. ETL: Loading the dataset\n",
    "\n",
    "**Note**: Here I chose to translate English to German, simply it is easier for myself, since I don't understand German so it is difficult for me to imagine a sentence during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Verah/JParaCrawl-Filtered-English-Japanese-Parallel-Corpus\", split=\"train\").select(range(30000)).remove_columns([\"id\", \"model1_accepted\", \"model2_accepted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'english': 'And everyone will not care that it is not you.',\n",
       " 'japanese': '鼻・口のところはあらかじめ少し切っておくといいですね。'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_LANGUAGE = 'english'\n",
    "TRG_LANGUAGE = 'japanese'\n",
    "\n",
    "data_list = [(item[SRC_LANGUAGE], item[TRG_LANGUAGE]) for item in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = type('CustomDataset', (Dataset,), {\n",
    "    '__len__': lambda self: len(data_list),\n",
    "    '__getitem__': lambda self, idx: data_list[idx]\n",
    "})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.2 * len(dataset))\n",
    "test_size = len(dataset) - (train_size + val_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. EDA - simple investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 29001 is plenty,, we gonna call `random_split` to train, val and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('And everyone will not care that it is not you.',\n",
       " '鼻・口のところはあらかじめ少し切っておくといいですね。')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(dataset))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = random_split(\n",
    "    dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(999)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size = len(list(iter(train)))\n",
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = len(list(iter(val)))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = len(list(iter(test)))\n",
    "test_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 3. Preprocessing \n",
    "\n",
    "### Tokenizing\n",
    "\n",
    "**Note**: the models must first be downloaded using the following on the command line: \n",
    "```\n",
    "python3 -m spacy download en_core_web_sm\n",
    "python3 -m spacy download de_core_news_sm\n",
    "```\n",
    "\n",
    "First, since we have two languages, let's create some constants to represent that.  Also, let's create two dicts: one for holding our tokenizers and one for holding all the vocabs with assigned numbers for each unique word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place-holders\n",
    "token_transform = {}\n",
    "vocab_transform = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_transform[SRC_LANGUAGE] = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "token_transform[TRG_LANGUAGE] = get_tokenizer('spacy', language='ja_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  And everyone will not care that it is not you.\n",
      "Tokenization:  ['And', 'everyone', 'will', 'not', 'care', 'that', 'it', 'is', 'not', 'you', '.']\n"
     ]
    }
   ],
   "source": [
    "#example of tokenization of the english part\n",
    "print(\"Sentence: \", sample[0])\n",
    "print(\"Tokenization: \", token_transform[SRC_LANGUAGE](sample[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to tokenize our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to yield list of tokens\n",
    "# here data can be `train` or `val` or `test`\n",
    "def yield_tokens(data, language):\n",
    "    language_index = {SRC_LANGUAGE: 0, TRG_LANGUAGE: 1}\n",
    "\n",
    "    for data_sample in data:\n",
    "        yield token_transform[language](data_sample[language_index[language]]) #either first or second index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we tokenize, let's define some special symbols so our neural network understand the embeddings of these symbols, namely the unknown, the padding, the start of sentence, and end of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special symbols and indices\n",
    "UNK_IDX, PAD_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "# Make sure the tokens are in order of their indices to properly insert them in vocab\n",
    "special_symbols = ['<unk>', '<pad>', '<sos>', '<eos>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Text to integers (Numericalization)\n",
    "\n",
    "Next we gonna create function (torchtext called vocabs) that turn these tokens into integers.  Here we use built in factory function <code>build_vocab_from_iterator</code> which accepts iterator that yield list or iterator of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    # Create torchtext's Vocab object \n",
    "    vocab_transform[ln] = build_vocab_from_iterator(yield_tokens(train, ln), \n",
    "                                                    min_freq=2,   #if not, everything will be treated as UNK\n",
    "                                                    specials=special_symbols,\n",
    "                                                    special_first=True) #indicates whether to insert symbols at the beginning or at the end                                            \n",
    "# Set UNK_IDX as the default index. This index is returned when the token is not found. \n",
    "# If not set, it throws RuntimeError when the queried token is not found in the Vocabulary. \n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    vocab_transform[ln].set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[433, 19, 11, 0, 11]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#see some example\n",
    "vocab_transform[SRC_LANGUAGE](['here', 'is', 'a', 'unknownword', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'visual'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we can reverse it....\n",
    "mapping = vocab_transform[SRC_LANGUAGE].get_itos()\n",
    "\n",
    "#print 1816, for example\n",
    "mapping[1891]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<unk>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try unknown vocab\n",
    "mapping[0]\n",
    "#they will all map to <unk> which has 0 as integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('<pad>', '<sos>', '<eos>')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's try special symbols\n",
    "mapping[1], mapping[2], mapping[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9728"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check unique vocabularies\n",
    "len(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Preparing the dataloader\n",
    "\n",
    "One thing we change here is the <code>collate_fn</code> which now also returns the length of sentence.  This is required for <code>packed_padded_sequence</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to club together sequential operations\n",
    "def sequential_transforms(*transforms):\n",
    "    def func(txt_input):\n",
    "        for transform in transforms:\n",
    "            txt_input = transform(txt_input)\n",
    "        return txt_input\n",
    "    return func\n",
    "\n",
    "# function to add BOS/EOS and create tensor for input sequence indices\n",
    "def tensor_transform(token_ids):\n",
    "    return torch.cat((torch.tensor([SOS_IDX]), \n",
    "                      torch.tensor(token_ids), \n",
    "                      torch.tensor([EOS_IDX])))\n",
    "\n",
    "# src and trg language text transforms to convert raw strings into tensors indices\n",
    "text_transform = {}\n",
    "for ln in [SRC_LANGUAGE, TRG_LANGUAGE]:\n",
    "    text_transform[ln] = sequential_transforms(token_transform[ln], #Tokenization\n",
    "                                               vocab_transform[ln], #Numericalization\n",
    "                                               tensor_transform) # Add BOS/EOS and create tensor\n",
    "\n",
    "\n",
    "# function to collate data samples into batch tesors\n",
    "def collate_batch(batch):\n",
    "    src_batch, src_len_batch, trg_batch = [], [], []\n",
    "    for src_sample, trg_sample in batch:\n",
    "        processed_text = text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\"))\n",
    "        src_batch.append(processed_text)\n",
    "        trg_batch.append(text_transform[TRG_LANGUAGE](trg_sample.rstrip(\"\\n\")))\n",
    "        src_len_batch.append(processed_text.size(0))\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first = True) #<----need this because we use linear layers mostly\n",
    "    trg_batch = pad_sequence(trg_batch, padding_value=PAD_IDX, batch_first = True)\n",
    "    return src_batch, torch.tensor(src_len_batch, dtype=torch.int64), trg_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create train, val, and test dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = BATCH_SIZE\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "valid_loader = DataLoader(val,   batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "test_loader  = DataLoader(test,  batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the train loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for en, _, ja in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English shape:  torch.Size([12, 81])\n",
      "Japanese shape:  torch.Size([12, 91])\n"
     ]
    }
   ],
   "source": [
    "print(\"English shape: \", en.shape)  # (batch_size, seq len)\n",
    "print(\"Japanese shape: \", ja.shape)   # (batch_size, seq len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Design the model\n",
    "\n",
    "<img src=\"../figures/transformer-encoder.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        #src = [batch size, src len, hid dim]\n",
    "        #src_mask = [batch size, 1, 1, src len]   #if the token is padding, it will be 1, otherwise 0\n",
    "        _src, _ = self.self_attention(src, src, src, src_mask)\n",
    "        src     = self.self_attn_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        _src    = self.feedforward(src)\n",
    "        src     = self.ff_layer_norm(src + self.dropout(_src))\n",
    "        #src: [batch_size, src len, hid dim]\n",
    "        \n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, n_layers, n_heads, pf_dim, dropout, device, max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(input_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                           for _ in range(n_layers)])\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(self.device)\n",
    "        \n",
    "    def forward(self, src, src_mask):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        src_len    = src.shape[1]\n",
    "        \n",
    "        pos        = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, src_len]\n",
    "        \n",
    "        src        = self.dropout((self.tok_embedding(src) * self.scale) + self.pos_embedding(pos))\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            src = layer(src, src_mask)\n",
    "        #src: [batch_size, src_len, hid_dim]\n",
    "        \n",
    "        return src\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutli Head Attention Layer\n",
    "\n",
    "<img src = \"../figures/transformer-attention.png\" width=\"700\">\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{Softmax} \\big( \\frac{QK^T}{\\sqrt{d_k}} \\big)V $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, dropout, device):\n",
    "        super().__init__()\n",
    "        assert hid_dim % n_heads == 0\n",
    "        self.hid_dim  = hid_dim\n",
    "        self.n_heads  = n_heads\n",
    "        self.head_dim = hid_dim // n_heads\n",
    "        \n",
    "        self.fc_q     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_k     = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc_v     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.fc_o     = nn.Linear(hid_dim, hid_dim)\n",
    "        \n",
    "        self.dropout  = nn.Dropout(dropout)\n",
    "        \n",
    "        self.scale    = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n",
    "                \n",
    "    def forward(self, query, key, value, mask = None):\n",
    "        #src, src, src, src_mask\n",
    "        #query = [batch size, query len, hid dim]\n",
    "        #key = [batch size, key len, hid dim]\n",
    "        #value = [batch size, value len, hid dim]\n",
    "        \n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        Q = self.fc_q(query)\n",
    "        K = self.fc_k(key)\n",
    "        V = self.fc_v(value)\n",
    "        #Q=K=V: [batch_size, src len, hid_dim]\n",
    "        \n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        #Q = [batch_size, n heads, query len, head_dim]\n",
    "        \n",
    "        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        #Q = [batch_size, n heads, query len, head_dim] @ K = [batch_size, n heads, head_dim, key len]\n",
    "        #energy = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        #for making attention to padding to 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, -1e10)\n",
    "            \n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        #attention = [batch_size, n heads, query len, key len]\n",
    "        \n",
    "        x = torch.matmul(self.dropout(attention), V)\n",
    "        #[batch_size, n heads, query len, key len] @ [batch_size, n heads, value len, head_dim]\n",
    "        #x = [batch_size, n heads, query len, head dim]\n",
    "        \n",
    "        x = x.permute(0, 2, 1, 3).contiguous()  #we can perform .view\n",
    "        #x = [batch_size, query len, n heads, head dim]\n",
    "        \n",
    "        x = x.view(batch_size, -1, self.hid_dim)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        x = self.fc_o(x)\n",
    "        #x = [batch_size, query len, hid dim]\n",
    "        \n",
    "        return x, attention\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position-wise Feedforward Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, pf_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(hid_dim, pf_dim)\n",
    "        self.fc2 = nn.Linear(pf_dim, hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = [batch size, src len, hid dim]\n",
    "        x = self.dropout(torch.relu(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Decoder Layer\n",
    "\n",
    "<img src = \"../figures/transformer-decoder.png\" >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, hid_dim, n_heads, pf_dim, dropout, device):\n",
    "        super().__init__()\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n",
    "        self.enc_attn_layer_norm  = nn.LayerNorm(hid_dim)\n",
    "        self.ff_layer_norm        = nn.LayerNorm(hid_dim)\n",
    "        self.self_attention       = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.encoder_attention    = MultiHeadAttentionLayer(hid_dim, n_heads, dropout, device)\n",
    "        self.feedforward          = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n",
    "        self.dropout              = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len, hid dim]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        _trg, _ = self.self_attention(trg, trg, trg, trg_mask)\n",
    "        trg     = self.self_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        _trg, attention = self.encoder_attention(trg, enc_src, enc_src, src_mask)\n",
    "        trg             = self.enc_attn_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        #attention = [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        _trg = self.feedforward(trg)\n",
    "        trg  = self.ff_layer_norm(trg + self.dropout(_trg))\n",
    "        #trg = [batch_size, trg len, hid dim]\n",
    "        \n",
    "        return trg, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hid_dim, n_layers, n_heads, \n",
    "                 pf_dim, dropout, device,max_length = 100):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tok_embedding = nn.Embedding(output_dim, hid_dim)\n",
    "        self.pos_embedding = nn.Embedding(max_length, hid_dim)\n",
    "        self.layers        = nn.ModuleList([DecoderLayer(hid_dim, n_heads, pf_dim, dropout, device)\n",
    "                                            for _ in range(n_layers)])\n",
    "        self.fc_out        = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout       = nn.Dropout(dropout)\n",
    "        self.scale         = torch.sqrt(torch.FloatTensor([hid_dim])).to(device)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len    = trg.shape[1]\n",
    "        \n",
    "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
    "        #pos: [batch_size, trg len]\n",
    "        \n",
    "        trg = self.dropout((self.tok_embedding(trg) * self.scale) + self.pos_embedding(pos))\n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg, attention = layer(trg, enc_src, trg_mask, src_mask)\n",
    "            \n",
    "        #trg: [batch_size, trg len, hid dim]\n",
    "        #attention: [batch_size, n heads, trg len, src len]\n",
    "        \n",
    "        output = self.fc_out(trg)\n",
    "        #output = [batch_size, trg len, output_dim]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together (become Seq2Seq!)\n",
    "\n",
    "Our `trg_sub_mask` will look something like this (for a target with 5 tokens):\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 0\\\\\n",
    "1 & 1 & 1 & 1 & 1\\\\\n",
    "\\end{matrix}$$\n",
    "\n",
    "The \"subsequent\" mask is then logically anded with the padding mask, this combines the two masks ensuring both the subsequent tokens and the padding tokens cannot be attended to. For example if the last two tokens were `<pad>` tokens the mask would look like:\n",
    "\n",
    "$$\\begin{matrix}\n",
    "1 & 0 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 0 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "1 & 1 & 1 & 0 & 0\\\\\n",
    "\\end{matrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTransformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_pad_idx, trg_pad_idx, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "        \n",
    "    def make_src_mask(self, src):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        \n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, trg):\n",
    "        \n",
    "        #trg = [batch size, trg len]\n",
    "        \n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        #trg_pad_mask = [batch size, 1, 1, trg len]\n",
    "        \n",
    "        trg_len = trg.shape[1]\n",
    "        \n",
    "        trg_sub_mask = torch.tril(torch.ones((trg_len, trg_len), device = self.device)).bool()\n",
    "        #trg_sub_mask = [trg len, trg len]\n",
    "            \n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \n",
    "        #src = [batch size, src len]\n",
    "        #trg = [batch size, trg len]\n",
    "                \n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        \n",
    "        #src_mask = [batch size, 1, 1, src len]\n",
    "        #trg_mask = [batch size, 1, trg len, trg len]\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        #enc_src = [batch size, src len, hid dim]\n",
    "                \n",
    "        output, attention = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        #output = [batch size, trg len, output dim]\n",
    "        #attention = [batch size, n heads, trg len, src len]\n",
    "        \n",
    "        return output, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(m):\n",
    "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
    "        nn.init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab_transform[SRC_LANGUAGE])\n",
    "OUTPUT_DIM = len(vocab_transform[TRG_LANGUAGE])\n",
    "HID_DIM = 256\n",
    "ENC_LAYERS = 3\n",
    "DEC_LAYERS = 3\n",
    "ENC_HEADS = 8\n",
    "DEC_HEADS = 8\n",
    "ENC_PF_DIM = 512\n",
    "DEC_PF_DIM = 512\n",
    "ENC_DROPOUT = 0.1\n",
    "DEC_DROPOUT = 0.1\n",
    "\n",
    "enc = Encoder(INPUT_DIM, \n",
    "              HID_DIM, \n",
    "              ENC_LAYERS, \n",
    "              ENC_HEADS, \n",
    "              ENC_PF_DIM, \n",
    "              ENC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, \n",
    "              HID_DIM, \n",
    "              DEC_LAYERS, \n",
    "              DEC_HEADS, \n",
    "              DEC_PF_DIM, \n",
    "              DEC_DROPOUT, \n",
    "              device)\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2SeqTransformer(\n",
       "  (encoder): Encoder(\n",
       "    (tok_embedding): Embedding(9728, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): EncoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (tok_embedding): Embedding(9301, 256)\n",
       "    (pos_embedding): Embedding(100, 256)\n",
       "    (layers): ModuleList(\n",
       "      (0): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): DecoderLayer(\n",
       "        (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (enc_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (encoder_attention): MultiHeadAttentionLayer(\n",
       "          (fc_q): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_k): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_v): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (fc_o): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (feedforward): PositionwiseFeedforwardLayer(\n",
       "          (fc1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (fc_out): Linear(in_features=256, out_features=9301, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim   = len(vocab_transform[SRC_LANGUAGE])\n",
    "output_dim  = len(vocab_transform[TRG_LANGUAGE])\n",
    "hid_dim = 256\n",
    "enc_layers = 3\n",
    "dec_layers = 3\n",
    "enc_heads = 8\n",
    "dec_heads = 8\n",
    "enc_pf_dim = 512\n",
    "dec_pf_dim = 512\n",
    "enc_dropout = 0.1\n",
    "dec_dropout = 0.1\n",
    "\n",
    "SRC_PAD_IDX = PAD_IDX\n",
    "TRG_PAD_IDX = PAD_IDX\n",
    "\n",
    "enc = Encoder(input_dim, \n",
    "              hid_dim, \n",
    "              enc_layers, \n",
    "              enc_heads, \n",
    "              enc_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "dec = Decoder(output_dim, \n",
    "              hid_dim, \n",
    "              dec_layers, \n",
    "              dec_heads, \n",
    "              dec_pf_dim, \n",
    "              enc_dropout, \n",
    "              device)\n",
    "\n",
    "model = Seq2SeqTransformer(enc, dec, SRC_PAD_IDX, TRG_PAD_IDX, device).to(device)\n",
    "model.apply(initialize_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2490368\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2381056\n",
      " 25600\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      " 65536\n",
      "   256\n",
      "131072\n",
      "   512\n",
      "131072\n",
      "   256\n",
      "2381056\n",
      "  9301\n",
      "______\n",
      "11266645\n"
     ]
    }
   ],
   "source": [
    "#we can print the complexity by the number of parameters\n",
    "def count_parameters(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    for item in params:\n",
    "        print(f'{item:>6}')\n",
    "    print(f'______\\n{sum(params):>6}')\n",
    "    \n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0005\n",
    "\n",
    "#training hyperparameters\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX) #combine softmax with cross entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we'll define our training loop. This is the exact same as the one used in the previous tutorial.\n",
    "\n",
    "As we want our model to predict the `<eos>` token but not have it be an input into our model we simply slice the `<eos>` token off the end of the sequence. Thus:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{trg} &= [sos, x_1, x_2, x_3, eos]\\\\\n",
    "\\text{trg[:-1]} &= [sos, x_1, x_2, x_3]\n",
    "\\end{align*}$$\n",
    "\n",
    "$x_i$ denotes actual target sequence element. We then feed this into the model to get a predicted sequence that should hopefully predict the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "$y_i$ denotes predicted target sequence element. We then calculate our loss using the original `trg` tensor with the `<sos>` token sliced off the front, leaving the `<eos>` token:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{output} &= [y_1, y_2, y_3, eos]\\\\\n",
    "\\text{trg[1:]} &= [x_1, x_2, x_3, eos]\n",
    "\\end{align*}$$\n",
    "\n",
    "We then calculate our losses and update our parameters as is standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, clip, loader_length):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for src, src_len, trg in loader:\n",
    "\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #trg[:, :-1] remove the eos, e.g., \"<sos> I love sushi\" since teaching forcing, the input does not need to have eos. \n",
    "        try:\n",
    "            output, _ = model(src, trg[:,:-1])\n",
    "        except:\n",
    "            continue\n",
    "                \n",
    "        #output = [batch size, trg len - 1, output dim]\n",
    "        #trg    = [batch size, trg len]\n",
    "            \n",
    "        output_dim = output.shape[-1]\n",
    "            \n",
    "        output = output.reshape(-1, output_dim)\n",
    "        trg = trg[:,1:].reshape(-1) #trg[:, 1:] remove the sos, e.g., \"i love sushi <eos>\" since in teaching forcing, the output does not have sos\n",
    "                \n",
    "        #output = [batch size * trg len - 1, output dim]\n",
    "        #trg    = [batch size * trg len - 1]\n",
    "            \n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our evaluation loop is similar to our training loop, however as we aren't updating any parameters we don't need to pass an optimizer or a clip value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, loader_length):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for src, src_len, trg in loader:\n",
    "        \n",
    "            src = src.to(device)\n",
    "            trg = trg.to(device)\n",
    "       \n",
    "            try:\n",
    "                output, _ = model(src, trg[:,:-1])\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            #output = [batch size, trg len - 1, output dim]\n",
    "            #trg = [batch size, trg len]\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output.contiguous().view(-1, output_dim)\n",
    "            trg = trg[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            #output = [batch size * trg len - 1, output dim]\n",
    "            #trg = [batch size * trg len - 1]\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / loader_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything together\n",
    "\n",
    "Finally, we train our actual model. This model is almost 3x faster than the convolutional sequence-to-sequence model and also achieves a lower validation perplexity!\n",
    "\n",
    "**Note: similar to CNN, this model always has a teacher forcing ratio of 1, i.e. it will always use the ground truth next token from the target sequence (this is simply because CNN do everything in parallel so we cannot have the next token). This means we cannot compare perplexity values against the previous models when they are using a teacher forcing ratio that is not 1. To understand this, try run previous tutorials with teaching forcing ratio of 1, you will get very low perplexity.  **   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_length = len(list(iter(train_loader)))\n",
    "val_loader_length   = len(list(iter(valid_loader)))\n",
    "test_loader_length  = len(list(iter(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 9m 20s\n",
      "Train Loss: 1.545 | Train PPL:   4.688\n",
      "Val. Loss: 0.906 |  Val. PPL:   2.474\n",
      "Epoch: 02 | Time: 18m 42s\n",
      "Train Loss: 0.899 | Train PPL:   2.456\n",
      "Val. Loss: 0.827 |  Val. PPL:   2.286\n",
      "Epoch: 03 | Time: 27m 47s\n",
      "Train Loss: 0.810 | Train PPL:   2.249\n",
      "Val. Loss: 0.796 |  Val. PPL:   2.218\n",
      "Epoch: 04 | Time: 37m 4s\n",
      "Train Loss: 0.739 | Train PPL:   2.093\n",
      "Val. Loss: 0.773 |  Val. PPL:   2.165\n",
      "Epoch: 05 | Time: 46m 13s\n",
      "Train Loss: 0.661 | Train PPL:   1.937\n",
      "Val. Loss: 0.770 |  Val. PPL:   2.159\n",
      "Epoch: 06 | Time: 55m 25s\n",
      "Train Loss: 0.649 | Train PPL:   1.914\n",
      "Val. Loss: 0.761 |  Val. PPL:   2.140\n",
      "Epoch: 07 | Time: 64m 37s\n",
      "Train Loss: 0.602 | Train PPL:   1.826\n",
      "Val. Loss: 0.765 |  Val. PPL:   2.149\n",
      "Epoch: 08 | Time: 73m 53s\n",
      "Train Loss: 0.564 | Train PPL:   1.757\n",
      "Val. Loss: 0.764 |  Val. PPL:   2.148\n",
      "Epoch: 09 | Time: 85m 30s\n",
      "Train Loss: 0.519 | Train PPL:   1.681\n",
      "Val. Loss: 0.775 |  Val. PPL:   2.170\n",
      "Epoch: 10 | Time: 101m 26s\n",
      "Train Loss: 0.485 | Train PPL:   1.623\n",
      "Val. Loss: 0.792 |  Val. PPL:   2.207\n",
      "Epoch: 11 | Time: 119m 12s\n",
      "Train Loss: 0.464 | Train PPL:   1.591\n",
      "Val. Loss: 0.790 |  Val. PPL:   2.204\n",
      "Epoch: 12 | Time: 138m 15s\n",
      "Train Loss: 0.434 | Train PPL:   1.544\n",
      "Val. Loss: 0.810 |  Val. PPL:   2.247\n",
      "Epoch: 13 | Time: 157m 8s\n",
      "Train Loss: 0.415 | Train PPL:   1.514\n",
      "Val. Loss: 0.825 |  Val. PPL:   2.281\n",
      "Epoch: 14 | Time: 175m 15s\n",
      "Train Loss: 0.385 | Train PPL:   1.470\n",
      "Val. Loss: 0.833 |  Val. PPL:   2.300\n",
      "Epoch: 15 | Time: 193m 18s\n",
      "Train Loss: 0.383 | Train PPL:   1.467\n",
      "Val. Loss: 0.842 |  Val. PPL:   2.321\n",
      "Epoch: 16 | Time: 211m 4s\n",
      "Train Loss: 0.349 | Train PPL:   1.418\n",
      "Val. Loss: 0.852 |  Val. PPL:   2.343\n",
      "Epoch: 17 | Time: 228m 54s\n",
      "Train Loss: 0.337 | Train PPL:   1.400\n",
      "Val. Loss: 0.873 |  Val. PPL:   2.394\n",
      "Epoch: 18 | Time: 246m 33s\n",
      "Train Loss: 0.327 | Train PPL:   1.387\n",
      "Val. Loss: 0.883 |  Val. PPL:   2.419\n",
      "Epoch: 19 | Time: 264m 12s\n",
      "Train Loss: 0.305 | Train PPL:   1.356\n",
      "Val. Loss: 0.884 |  Val. PPL:   2.422\n",
      "Epoch: 20 | Time: 281m 42s\n",
      "Train Loss: 0.297 | Train PPL:   1.345\n",
      "Val. Loss: 0.900 |  Val. PPL:   2.461\n",
      "Epoch: 21 | Time: 299m 25s\n",
      "Train Loss: 0.286 | Train PPL:   1.331\n",
      "Val. Loss: 0.914 |  Val. PPL:   2.495\n",
      "Epoch: 22 | Time: 316m 57s\n",
      "Train Loss: 0.277 | Train PPL:   1.319\n",
      "Val. Loss: 0.921 |  Val. PPL:   2.512\n",
      "Epoch: 23 | Time: 334m 12s\n",
      "Train Loss: 0.264 | Train PPL:   1.303\n",
      "Val. Loss: 0.943 |  Val. PPL:   2.567\n",
      "Epoch: 24 | Time: 351m 40s\n",
      "Train Loss: 0.257 | Train PPL:   1.294\n",
      "Val. Loss: 0.944 |  Val. PPL:   2.571\n",
      "Epoch: 25 | Time: 369m 9s\n",
      "Train Loss: 0.257 | Train PPL:   1.293\n",
      "Val. Loss: 0.955 |  Val. PPL:   2.598\n",
      "Epoch: 26 | Time: 386m 10s\n",
      "Train Loss: 0.245 | Train PPL:   1.278\n",
      "Val. Loss: 0.965 |  Val. PPL:   2.624\n",
      "Epoch: 27 | Time: 403m 40s\n",
      "Train Loss: 0.230 | Train PPL:   1.259\n",
      "Val. Loss: 0.977 |  Val. PPL:   2.658\n",
      "Epoch: 28 | Time: 421m 29s\n",
      "Train Loss: 0.238 | Train PPL:   1.269\n",
      "Val. Loss: 0.988 |  Val. PPL:   2.687\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 14\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     valid_loss \u001b[38;5;241m=\u001b[39m evaluate(model, valid_loader, criterion, val_loader_length)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m#for plotting\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, criterion, clip, loader_length)\u001b[0m\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, trg)\n\u001b[0;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 35\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     39\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:42\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite)\u001b[0m\n\u001b[0;32m     40\u001b[0m     total_norm \u001b[38;5;241m=\u001b[39m norms[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(norms) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mstack(norms))\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdetach(), norm_type)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters]), norm_type)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_norm\u001b[38;5;241m.\u001b[39misnan() \u001b[38;5;129;01mor\u001b[39;00m total_norm\u001b[38;5;241m.\u001b[39misinf():\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite:\n",
      "File \u001b[1;32mc:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     40\u001b[0m     total_norm \u001b[38;5;241m=\u001b[39m norms[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(norms) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mstack(norms))\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     total_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters]), norm_type)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_norm\u001b[38;5;241m.\u001b[39misnan() \u001b[38;5;129;01mor\u001b[39;00m total_norm\u001b[38;5;241m.\u001b[39misinf():\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m error_if_nonfinite:\n",
      "File \u001b[1;32mc:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\functional.py:1312\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(p, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1311\u001b[0m         _dim \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ndim)]  \u001b[38;5;66;03m# noqa: C416 TODO: rewrite as list(range(m))\u001b[39;00m\n\u001b[1;32m-> 1312\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdim\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m \u001b[38;5;66;03m# TODO: when https://github.com/pytorch/pytorch/issues/33782 is fixed\u001b[39;00m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;66;03m# remove the overloads where dim is an int and replace with BraodcastingList1\u001b[39;00m\n\u001b[0;32m   1316\u001b[0m \u001b[38;5;66;03m# and remove next four lines, replace _dim with dim\u001b[39;00m\n\u001b[0;32m   1317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "num_epochs = 40\n",
    "clip       = 1\n",
    "\n",
    "save_path = f'../../app/models/transformer/{model.__class__.__name__}.pt'\n",
    "\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip, train_loader_length)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion, val_loader_length)\n",
    "    \n",
    "    #for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, time.time())\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "    \n",
    "    #lower perplexity is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(vocab_transform[SRC_LANGUAGE].get_default_index())  # Should be UNK_IDX (0)\n",
    "print(vocab_transform[TRG_LANGUAGE].get_default_index())  # Should be UNK_IDX (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max src index: tensor(8297)\n",
      "Max trg index: tensor(9280)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    src, _, trg = batch\n",
    "    print(\"Max src index:\", torch.max(src))\n",
    "    print(\"Max trg index:\", torch.max(trg))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source vocab size: 9728\n",
      "Target vocab size: 9301\n"
     ]
    }
   ],
   "source": [
    "print(f\"Source vocab size: {len(vocab_transform[SRC_LANGUAGE])}\")\n",
    "print(f\"Target vocab size: {len(vocab_transform[TRG_LANGUAGE])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dim: 9728, Output dim: 9301\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input dim: {INPUT_DIM}, Output dim: {OUTPUT_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'loss')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(5, 3))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(train_losses, label = 'train loss')\n",
    "ax.plot(valid_losses, label = 'valid loss')\n",
    "plt.legend()\n",
    "ax.set_xlabel('updates')\n",
    "ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.810 | Test PPL:   2.249 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "test_loss = evaluate(model, test_loader, criterion, test_loader_length)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test on some random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'And everyone will not care that it is not you.'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'鼻・口のところはあらかじめ少し切っておくといいですね。'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,  306,  656,   54,   90, 1520,   28,   37,   19,   90,   17,    5,\n",
       "           3])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text = text_transform[SRC_LANGUAGE](sample[0]).to(device)\n",
    "src_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2, 3969,   64,  823,    4, 1097,   10, 2470, 1072, 2666,    9, 1091,\n",
       "          13,  300,   27, 1038,    8,    3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_text = text_transform[TRG_LANGUAGE](sample[1]).to(device)\n",
    "trg_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_text = src_text.reshape(1, -1)  #because batch_size is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_text = trg_text.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 13]), torch.Size([1, 18]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_text.shape, trg_text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_length = torch.tensor([src_text.size(0)]).to(dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output, attentions = model(src_text, trg_text) #turn off teacher forcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 9301])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape #batch_size, trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since batch size is 1, we just take off that dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 9301])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall remove the first token since it's zeroes anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 9301])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = output[1:]\n",
    "output.shape #trg_len, trg_output_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we just take the top token with highest probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_max = output.argmax(1) #returns max indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 33,   0,  14, 235,  14,   5,   0,  14,   9,  42,  13, 512,   8,   8,\n",
       "          8,   3,   8])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the mapping of the target language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = vocab_transform[TRG_LANGUAGE].get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "から\n",
      "<unk>\n",
      "で\n",
      "場合\n",
      "で\n",
      "、\n",
      "<unk>\n",
      "で\n",
      "て\n",
      "いる\n",
      "と\n",
      "思い\n",
      "。\n",
      "。\n",
      "。\n",
      "<eos>\n",
      "。\n"
     ]
    }
   ],
   "source": [
    "for token in output_max:\n",
    "    print(mapping[token.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Attention\n",
    "\n",
    "Let's display the attentions to understand how the source text links with the generated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 18, 13])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attentions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 8 heads, we can look at just 1 head for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([18, 13])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention = attentions[0, 0, :, :]\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'And',\n",
       " 'everyone',\n",
       " 'will',\n",
       " 'not',\n",
       " 'care',\n",
       " 'that',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'you',\n",
       " '.',\n",
       " '<eos>']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tokens = ['<sos>'] + token_transform[SRC_LANGUAGE](sample[0]) + ['<eos>']\n",
    "src_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos>',\n",
       " 'から',\n",
       " '<unk>',\n",
       " 'で',\n",
       " '場合',\n",
       " 'で',\n",
       " '、',\n",
       " '<unk>',\n",
       " 'で',\n",
       " 'て',\n",
       " 'いる',\n",
       " 'と',\n",
       " '思い',\n",
       " '。',\n",
       " '。',\n",
       " '。',\n",
       " '<eos>',\n",
       " '。']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_tokens = ['<sos>'] + [mapping[token.item()] for token in output_max]\n",
    "trg_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_attention(sentence, translation, attention):\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
    "    \n",
    "    cax = ax.matshow(attention, cmap='bone')\n",
    "   \n",
    "    ax.tick_params(labelsize=10)\n",
    "    \n",
    "    y_ticks =  [''] + translation\n",
    "    x_ticks =  [''] + sentence \n",
    "     \n",
    "    ax.set_xticklabels(x_ticks, rotation=45)\n",
    "    ax.set_yticklabels(y_ticks)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\swara\\AppData\\Local\\Temp\\ipykernel_2512\\140266900.py:15: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(x_ticks, rotation=45)\n",
      "C:\\Users\\swara\\AppData\\Local\\Temp\\ipykernel_2512\\140266900.py:16: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_yticklabels(y_ticks)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12363 (\\N{HIRAGANA LETTER KA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12425 (\\N{HIRAGANA LETTER RA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12391 (\\N{HIRAGANA LETTER DE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 22580 (\\N{CJK UNIFIED IDEOGRAPH-5834}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 21512 (\\N{CJK UNIFIED IDEOGRAPH-5408}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12289 (\\N{IDEOGRAPHIC COMMA}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12390 (\\N{HIRAGANA LETTER TE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12356 (\\N{HIRAGANA LETTER I}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12427 (\\N{HIRAGANA LETTER RU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12392 (\\N{HIRAGANA LETTER TO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 24605 (\\N{CJK UNIFIED IDEOGRAPH-601D}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "c:\\Users\\swara\\anaconda3\\envs\\nlp\\lib\\site-packages\\IPython\\core\\pylabtools.py:152: UserWarning: Glyph 12290 (\\N{IDEOGRAPHIC FULL STOP}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_attention(src_tokens, trg_tokens, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "714d3f4db9a58ba7d2f2a9a4fffe577af3df8551aebd380095064812e2e0a6a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
